

---
title: "data_analysis_for_McManus"
author: "Siqi"
date: "`r Sys.Date()`"
output:
  html_document: default
  pdf_document:
    latex_engine: xelatex
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

```{r }
# Load necessary libraries 
set.seed(123)
library(ggplot2)
library(splines)
library(minpack.lm)
#install.packages("pracma") 
library(pracma)
#install.packages("nls2")
library(nls2)



```

## Including Plots

Data Cleaning and Plotting

```{r  }
# Load and process the  
library(readxl)
Data_McManus2_WT_P2L_AAVCremCherry <- read_excel("Data_McManus2_WT_P2L_AAVCremCherry.xlsx")


df_MC <- as.data.frame(Data_McManus2_WT_P2L_AAVCremCherry)

# Rename columns for clarity
colnames(df_MC) <- c("Time", "X1", "X2", "X3", "X4")

# Remove outliers where Time is between 100 and 100.1
df_MC <- df_MC[!(df_MC$Time >= 100 & df_MC$Time <= 100.1), ]

# Plot each species
ggplot(df_MC, aes(x = Time)) +
  geom_line(aes(y = X1, color = "X1")) +
  geom_line(aes(y = X2, color = "X2")) +
  geom_line(aes(y = X3, color = "X3")) +
  geom_line(aes(y = X4, color = "X4")) +
  labs(title = "Data without Outliers", x = "Time", y = "X(t_i)") +
  scale_color_manual(values = c("X1" = "blue", "X2" = "green", "X3" = "red", "X4" = "purple")) +
  theme_minimal()

```

```{r}

spline_X1 <- predict(smooth.spline(df_MC$Time, df_MC$X1),    spar =)
spline_X2 <- predict(smooth.spline(df_MC$Time, df_MC$X2))
spline_X3 <- predict(smooth.spline(df_MC$Time, df_MC$X3))
spline_X4 <- predict(smooth.spline(df_MC$Time,df_MC$X4))

# Plot the spline-smoothed data
plot(df_MC$Time, spline_X1$y, type = "l", col = "red", lwd = 2, 
     xlab = "Time", ylab = "Values", main = " Splines", 
     ylim = range(c(spline_X1$y, spline_X2$y, spline_X3$y, spline_X4)))
lines(df_MC$Time, spline_X2$y, col = "blue", lwd = 2)
lines(df_MC$Time, spline_X3$y, col = "green", lwd = 2)
lines(df_MC$Time, spline_X4$y, col = "purple", lwd = 2)

legend("topright", legend = c("spline_X1", "spline_X2", "spline_X3", "spline_X4"), 
       col = c("red", "blue", "green", "purple"), lwd = 2)


# Plot the spline-smoothed data
spline_X1 <- predict(smooth.spline(df_MC$Time, df_MC$X1, spar=1)) #black line in 2nd #plot（average for X1）

plot(df_MC$Time, df_MC$X1, type = "l", col = "yellow", lwd = 2, 
     xlab = "Time", ylab = "Values", main = " Splines", 
     ylim = range(c(spline_X1$y, spline_X2$y, spline_X3$y, spline_X4)))
lines(df_MC$Time, spline_X1$y)

plot(df_MC$Time, df_MC$X1-spline_X1$y, type = "l", col = "yellow", lwd = 2, 
     xlab = "Time", ylab = "Values", main = " Detrended Splines")



```
```{r}
x <- 1:length(df_MC$Time)  
y <- df_MC$X1-spline_X1$y    

maxima <- findpeaks(df_MC$X1-spline_X1$y, minpeakheight=0)  
minima <- findpeaks(-df_MC$X1-spline_X1$y, minpeakheight=0) 

x_max <- maxima[,2]  
y_max <- maxima[,1]  


fit <- nls(y_max ~ a * exp(-b * x_max) + c, start = list(a = max(y_max), b = 0.01, c = min(y_max)))

summary(fit)

a <- coef(fit)["a"]
b <- coef(fit)["b"]
c <- coef(fit)["c"]

plot(x, y, type="l", col="yellow", main="Detrended Splines with Exponential Fit")
lines(x_max, predict(fit, list(x_max = x_max)), col="red", lwd=2)  #

```



Simulate Data Based on the Fitted Model

```{r}
```


```{r}
t <- seq(0, 100, by = 0.1)

model_function <- function(t, A, gamma, omega, phi, y_shift) {
  A * exp(-gamma * t ) * cos(omega * t + phi) + y_shift
}

# using estimated parameters to def simulating function
simulate_data <- function(t, params, sigma) {
  predicted <- model_function(t, params[1], params[2], params[3], params[4], params[5])
  noise <- rnorm(length(t), mean = 0, sd = sigma)  #using sigma0 to produce noise 
  simulated_data <- predicted + noise
  return(simulated_data)
}
sigma0 = 10
param0=c(1000, 0.025, 0.9, 0,0)
simulated_data <- simulate_data(t, param0, sigma0 )
```


```{r}
plot(t, simulated_data, type = "l", col = "blue", main = "Simulated Data", xlab = "Time", ylab = "Simulated Values")
```





## Task1
## generate simulating data 1 time

```{r}


true_params <- c(A = 1000, gamma = 0.025, omega = 0.9, phi = 0, y_shift = 0)

sigma0 <- 10
t <- seq(0, 100, by = 0.1)
set.seed(123)
y_sim <- model_function(t, true_params[1], true_params[2], true_params[3], true_params[4], true_params[5]) + 
         rnorm(length(t), mean = 0, sd = sigma0)

# default algorithm in nls: Gauss-Newton
fit_nls <- nls(
  y_sim ~ model_function(t, A, gamma, omega, phi, y_shift),
  start = list(A = 950, gamma = 0.03, omega = 0.85, phi = 0.1, y_shift = 0), #initial value close to true ones
  data = data.frame(t = t, y_sim = y_sim)
)
# 
summary(fit_nls)
coef(fit_nls)
```

##Task 2 find peaks
```{r}
t <- seq(0, 100, by = 0.1)
# find peaks
peaks <- findpeaks(y_sim, minpeakheight = 100, minpeakdistance = 15) 

# peaks[, 2] :index of peaks in (1~1000)
#t[ peaks[, 2]]: index of peaks in(1~100)
t_peaks <- t[peaks[, 2]] 

y_peaks <- peaks[, 1]

#
plot(t, y_sim, type = "l", col = "blue", main = "Simulated Data with Peaks", xlab = "Time", ylab = "Simulated Values")
points(t_peaks, y_peaks, col = "red", pch = 19)
```
## Finding initial parameters:A and gamma for simulated data
```{r}
# transform into log
log_y_peaks <- log(y_peaks)

# logy = -gamma*t + logA
fit_linear <- lm(log_y_peaks ~ t_peaks) 

summary(fit_linear)
print("coef:")
coef(fit_linear)
coef(fit_linear)[1]
print("coef:")

# get log(A)  and  gamma
log_A <- coef(fit_linear)[1]

init_gamma <- -coef(fit_linear)[2] #Extract gamma from the linear fit

cat("log(A) =", log_A, "\n")
cat("gamma =", init_gamma, "\n")
init_A=exp(log_A)
print(init_A)
```

## to see whether the initial params A and gamma is fitted or not
```{r}
#Plot the simulated data
plot(t, y_sim, type = "l", col = "blue", main = "Simulated Data with Fitted Exponential", xlab = "Time", ylab = "Simulated Values")

# Calculate the fitted exponential curve using estimated A and gamma from the linear model
init_A <- exp(log_A)  # Extract log_A from the linear model

# Add the fitted exponential curve
curve(init_A * exp(-init_gamma * x), from = 0, to = max(t), col = "green", add = TRUE, lwd = 2)
#  Add a legend 
legend("topright", legend = c("Simulated Data", "Fitted Exponential"), col = c("blue", "green"), lty = 1)

```

## find initial value：omega
```{r}
# Extract the time points of the first two consecutive maxima from t_peaks
t_max1 <- t_peaks[1]  # First maximum
t_max2 <- t_peaks[2]  # Second maximum
t_max3 <- t_peaks[3] 
t_max4 <- t_peaks[4] 
# Calculate the time difference between two consecutive maxima
Delta <- t_max2 - t_max1
Delta 
print(t_max3-t_max2)
print(t_max4 - t_max3)
# Estimate omega (tau) using the formula 2*pi/Delta
init_omega <- 2 * pi / Delta

# Output the result
cat("initial omega (tau) =", init_omega, "\n")
cat("Time difference between consecutive maxima (Delta) =", Delta, "\n")

```
we assume period delta keeps same. now our initial parameters are[A = 968.7701,gamma = 0.02295209 ,omega = 0.9377889 , phi = 0, y_shift = 0]

```{r}
library(nlme)
  
fit_nls <- nls(
  y_sim ~ model_function(t, A, gamma, omega, phi, y_shift),
  start = list(A = init_A, gamma = init_gamma, omega = init_omega, phi = 0.1, y_shift = 0),
  data = data.frame(t = t, y_sim = y_sim)
)
fit_nls

```

## method used inside vcov()
The code below manually calculates the covariance matrix of a linear regression model and verifies its consistency with the automatically computed result from the `lm` function.

Specifically, it first generates data and constructs the design matrix \(X\). Then, it manually computes the regression coefficients using the formula 
\[
\hat{\beta} = (X^T X)^{-1} X^T y.
\]

Next, the covariance matrix is calculated with the formula 
\[
\text{Cov}(\hat{\beta}) = (X^T X)^{-1} \cdot \frac{\sum \text{resid}^2}{\text{df.res}},
\]
where \(\text{resid} = y - X\hat{\beta}\) represents the residuals and \(\text{df.res}\) is the degrees of freedom, used to adjust the error variance.

Finally, `all.equal` is used to compare the manually computed covariance matrix with the result from `vcov(mod)`, confirming their consistency, with the result `TRUE` indicating agreement.


```{r}
library(data.table)
df     = data.table(y = runif(100, 0, 100),  #generates 100 random deviates. for 0<x,y,z<100
                    x = runif(100, 0, 100),
                    z = runif(100, 0, 100))
mod    = lm(y ~ ., df)
X      = cbind(1, as.matrix(df[, -1])) #only include x and z
invXtX = solve(crossprod(X)) # compute for inverse of crossproduct(X^t*X)
coef   = invXtX %*% t(X) %*% df$y # compute coef manually: coef = (X^t*X)^(-1) *X^t*y
resid  = df$y - X %*% coef # residual = real value - predict value

df.res = nrow(X) - ncol(X) #degree of freedom
manual = invXtX * sum(resid**2)/(df.res) # 
funct  = vcov(mod)

print(mod)

all.equal(manual, funct, check.attributes = F)

```



## Function of simulate_and_estimate: used for repeat simulating
```{r}
#install.packages("nlstools")
library(nlstools)
library(stats)
simulate_and_estimate <- function(true_params, sigma0 = 10) {
  #simulate data
  t <- seq(0, 100, by = 0.1)
  model_function <- function(t, A, gamma, omega, phi, y_shift) {
    A * exp(-gamma * t) * cos(omega * t + phi) + y_shift
  }
  y_sim <- model_function(t, true_params[1], true_params[2], true_params[3], true_params[4], true_params[5]) +
    rnorm(length(t), mean = 0, sd = sigma0)

  # find initial value
  peaks <- findpeaks(y_sim, minpeakheight = 100, minpeakdistance = 15)
  t_peaks <- t[peaks[, 2]]
  y_peaks <- peaks[, 1]

  log_y_peaks <- log(y_peaks)
  fit_linear <- lm(log_y_peaks ~ t_peaks)
  log_A <- coef(fit_linear)[1]
  init_gamma <- -coef(fit_linear)[2]
  init_A <- exp(log_A)


  Delta <- t_peaks[2] - t_peaks[1]
  init_omega <- 2 * pi / Delta

    # find estimated parameters
  fit_nls <- try(nls(
    y_sim ~ model_function(t, A, gamma, omega, phi, y_shift),
    start = list(A = init_A, gamma = init_gamma, omega = init_omega, phi = 0.1, y_shift = 0),
    data = data.frame(t = t, y_sim = y_sim)
  ), silent = TRUE)
  #print(summary(fit_nls))

  #print(summary(fit_nls)$cov.unscaled)
  #cov_matrix1 <- summary(fit_nls)$sigma^2 * summary(fit_nls)$cov.unscaled
  #print(cov_matrix1)
  #print(vcov(fit_nls))
  # check if fitted successful
  if (class(fit_nls) == "try-error") {
    return(NULL)
  }
  # find estmate parameters and standard errors
  param_estimates <- coef(fit_nls)
  
  
  # Calculate standard errors and confidence intervals
  conf_intervals <- confint2(fit_nls, level = 0.99, method = 'asymptotic')
  return(list(estimates = param_estimates, conf_intervals = conf_intervals))
}
# test function
true_params <- c(A = 1000, gamma = 0.025, omega = 0.9, phi = 0, y_shift = 0)
result <- simulate_and_estimate(true_params)
result
```
## Task 3: repeat simulations
```{r}
# simulate 100
n_simulations <- 100
results <- vector("list", n_simulations)
# 
for (i in 1:n_simulations) {
  results[[i]] <- simulate_and_estimate(true_params)
}
results <- Filter(Negate(is.null), results)
null_count <- sum(sapply(results, is.null))
cat("Number of unsuccessful fits (NULL results):", null_count, "\n")
# Initialize coverage counters, one counter for each parameter
coverage_counts <- rep(0, length(true_params))

# Iterate through each simulation result to check coverage.
for (i in 1:length(results)) {
  ci <- results[[i]]$conf_intervals
  for (j in 1:length(true_params)) {
    # Check if it falls within the confidence interval.
    if (true_params[j] >= ci[j, 1] && true_params[j] <= ci[j, 2]) {
      coverage_counts[j] <- coverage_counts[j] + 1
    }
  }
}
#calculate coverage
coverage_proportions <- coverage_counts / n_simulations
# print unsuccessful fits


cat("Coverage for A:", coverage_proportions[1], "\n")
cat("Coverage for gamma:", coverage_proportions[2], "\n")
cat("Coverage for omega:", coverage_proportions[3], "\n")
cat("Coverage for phi:", coverage_proportions[4], "\n")
cat("Coverage for y_shift:", coverage_proportions[5], "\n")

```

## boxplot of estimated parames for 100 simulations

```{r}
library(tidyverse) 
library(dplyr)
library(tidyr)

# Extract valid results (remove NULL values);
valid_results <- Filter(Negate(is.null), results)

# Extract parameter estimates into a data frame
estimate_data <- do.call(rbind, lapply(valid_results, function(res) {
  as.data.frame(t(res$estimates))
}))


colnames(estimate_data) <- c("A", "gamma", "omega", "phi", "y_shift")

estimate_long <- estimate_data %>%
  pivot_longer(cols = everything(), names_to = "Parameter", values_to = "Estimate")

#install.packages("gridExtra")
library(gridExtra) # arange multiple plots

# Create separate boxplots for each parameter with suitable y-axis ranges;
plot_A <- ggplot(estimate_long %>% filter(Parameter == "A"), 
                 aes(x = Parameter, y = Estimate)) +
  geom_boxplot(fill = "skyblue", color = "darkblue") +
  geom_hline(yintercept = true_params["A"], linetype = "dashed") +
  labs(title = "A", y = "Estimate") +
  theme_minimal() +
  ylim(995, 1005)

plot_gamma <- ggplot(estimate_long %>% filter(Parameter == "gamma"), 
                     aes(x = Parameter, y = Estimate)) +
  geom_boxplot(fill = "lightgreen", color = "darkgreen") +
  geom_hline(yintercept = true_params["gamma"], linetype = "dashed") +
  labs(title = "gamma", y = "Estimate") +
  theme_minimal() +
  ylim(0.0248, 0.02515)

plot_omega <- ggplot(estimate_long %>% filter(Parameter == "omega"), 
                     aes(x = Parameter, y = Estimate)) +
  geom_boxplot(fill = "lightblue", color = "blue") +
  geom_hline(yintercept = true_params["omega"], linetype = "dashed") +
  labs(title = "omega", y = "Estimate") +
  theme_minimal() +
  ylim(0.89975, 0.90025)

plot_phi <- ggplot(estimate_long %>% filter(Parameter == "phi"), 
                   aes(x = Parameter, y = Estimate)) +
  geom_boxplot(fill = "pink", color = "red") +
  geom_hline(yintercept = true_params["phi"], linetype = "dashed") +
  labs(title = "phi", y = "Estimate") +
  theme_minimal() +
  ylim(min(estimate_data$phi) * 0.9, max(estimate_data$phi) * 1.1)

plot_y_shift <- ggplot(estimate_long %>% filter(Parameter == "y_shift"), 
                       aes(x = Parameter, y = Estimate)) +
  geom_boxplot(fill = "yellow", color = "orange") +
  geom_hline(yintercept = true_params["y_shift"], linetype = "dashed") +
  labs(title = "y_shift", y = "Estimate") +
  theme_minimal() +
  ylim(min(estimate_data$y_shift) * 0.9, max(estimate_data$y_shift) * 1.1)

# Arrange all subplots into a single large plot.
grid.arrange(plot_A, plot_gamma, plot_omega, plot_phi, plot_y_shift, ncol = 3)
```
This plot indicates that most of the estimated parameters obtained from 100 simulations are close to the true parameters.True parameters are indicated as blacked dashed line in above plot.

## Task 4 

finding initial parameters on asymmetric osillator model(on initial meeting file)
```{r}
time <- df_MC$Time
X1 <- df_MC$X1
# FInd X1 valleys and peaks
peaks <- findpeaks(X1, minpeakheight = 600, minpeakdistance = 20)
peaks<-peaks[-6, ]
t_peaks <- time[peaks[, 2]]
y_peaks<- peaks[, 1]
#delete outlier

valleys <- findpeaks(-X1, minpeakheight = -500, minpeakdistance = 50)

t_valleys <- time[valleys[, 2]]
y_valleys <- -valleys[, 1]  # restore 

# 
df_peaks <- data.frame(Time = t_peaks, Value = y_peaks)
df_valleys <- data.frame(Time = t_valleys, Value = y_valleys)


ggplot(df_MC, aes(x = Time, y = X1)) +
  geom_line(color = "blue") +
  geom_point(data = df_peaks, aes(x = Time, y = Value), color = "red", size = 2, shape = 17) +  
  geom_point(data = df_valleys, aes(x = Time, y = Value), color = "green", size = 2, shape = 18) + 
  labs(title = "X1 Data with Peaks and Valleys",
       x = "Time",
       y = "X1(t)") +
  theme_minimal()

```



```{r}
log_y_peaks <- log(y_peaks)
log_y_valleys <- log(y_valleys)

fit_peaks <- lm(log_y_peaks ~ t_peaks)
fit_valleys <- lm(log_y_valleys ~ t_valleys)

eta_max <- coef(fit_peaks)[2]  # η_max
eta_min <- coef(fit_valleys)[2]  # η_min

# estimate A_max and A_min
A_max <- exp(coef(fit_peaks)[1])
A_min <- exp(coef(fit_valleys)[1])


Delta <- t_peaks[2] - t_peaks[1]
tau <- 2 * pi / Delta

#assume phi = 0
phi <- 0

# print
cat("Initial A_max:", A_max, "\n")
cat("Initial A_min:", A_min, "\n")
cat("Initial eta_max:", eta_max, "\n")-
cat("Initial eta_min:", eta_min, "\n")
cat("Initial tau:", tau, "\n")
cat("Initial phi:", phi, "\n")

```
## plot fitted curve to compare
```{r}

max_exp_curve <- A_max * exp(eta_max * time)
min_exp_curve <- A_min * exp(eta_min * time)
# 
df_peaks <- data.frame(Time = t_peaks, Value = y_peaks)
df_valleys <- data.frame(Time = t_valleys, Value = y_valleys)
df_max_exp_curve <- data.frame(Time = time, ExponentialCurve = max_exp_curve)
df_min_exp_curve <- data.frame(Time = time, ExponentialCurve = min_exp_curve)
# 
ggplot(df_MC, aes(x = Time, y = X1)) +
  geom_line(color = "blue", size = 1) +  # 
  geom_line(data = df_max_exp_curve, aes(x = Time, y = ExponentialCurve), color = "purple",  linewidth = 1, linetype = "dashed") +  # expontential curve
  geom_line(data = df_min_exp_curve, aes(x = Time, y = ExponentialCurve), color = "yellow",  linewidth = 1, linetype = "dashed") +  # expontential curve
  geom_point(data = df_peaks, aes(x = Time, y = Value), color = "red", linewidth = 2, shape = 17) +  #
  geom_point(data = df_valleys, aes(x = Time, y = Value), color = "green", linewidth = 2, shape = 18) +
  labs(title = "X1 Data with Exponential Curve Fitting",
       x = "Time",
       y = "X1(t)") +
  theme_minimal() +
  scale_y_continuous(limits = c(0, max(X1) * 1.1))  #


```
using lm(),I cannot get a good fit, 
so I try to use nls to find initial A and gamma：
```{r}
nls_fit <- nls(y_peaks ~ A_max * exp(eta_max * t_peaks), 
               start = list(A_max = max(y_peaks), eta_max = -0.01), 
               control = nls.control(maxiter = 100))


A_max <- coef(nls_fit)["A_max"]
eta_max <- coef(nls_fit)["eta_max"]

# generate curve
exp_curve <- A_max * exp(eta_max * time)


df_exp_curve <- data.frame(Time = time, ExponentialCurve = exp_curve)
ggplot(df_MC, aes(x = Time, y = X1)) +
  geom_line(color = "blue", size = 1) +
  geom_line(data = df_exp_curve, aes(x = Time, y = ExponentialCurve), color = "purple", size = 1, linetype = "dashed") +
  geom_point(data = df_peaks, aes(x = Time, y = Value), color = "red", size = 2, shape = 17) +
  labs(title = "X1 Data with Improved Exponential Curve Fitting",
       x = "Time",
       y = "X1(t)") +
  theme_minimal() +
  scale_y_continuous(limits = c(0, max(X1) * 1.1))

```
still not good fit，

try to fit in 2 segments time before 100 and time after 100
```{r}
#  split data for early(t<100) and late segments(t>=100)
time_early <- time[time < 100]
time_late <- time[time >= 100]

y_peaks_early <- y_peaks[t_peaks < 100]
t_peaks_early <- t_peaks[t_peaks < 100]
y_valleys_early <- y_valleys[t_valleys < 100]
t_valleys_early <- t_valleys[t_valleys < 100]

y_peaks_late <- y_peaks[t_peaks >= 100]
t_peaks_late <- t_peaks[t_peaks >= 100]
y_valleys_late <- y_valleys[t_valleys >= 100]
t_valleys_late <- t_valleys[t_valleys >= 100]

# exponential fit when t <100
log_y_peaks_early <- log(y_peaks_early)
log_y_valleys_early <- log(y_valleys_early)

fit_peaks_early <- lm(log_y_peaks_early ~ t_peaks_early)
fit_valleys_early <- lm(log_y_valleys_early ~ t_valleys_early)

A_max_early <- exp(coef(fit_peaks_early)[1])
eta_max_early <- -coef(fit_peaks_early)[2]
A_min_early <- exp(coef(fit_valleys_early)[1])
eta_min_early <- -coef(fit_valleys_early)[2]

# exponential fit when t > 100
log_y_peaks_late <- log(y_peaks_late)
log_y_valleys_late <- log(y_valleys_late)

fit_peaks_late <- lm(log_y_peaks_late ~ t_peaks_late)
fit_valleys_late <- lm(log_y_valleys_late ~ t_valleys_late)

A_max_late <- exp(coef(fit_peaks_late)[1])
eta_max_late <- -coef(fit_peaks_late)[2]
A_min_late <- exp(coef(fit_valleys_late)[1])
eta_min_late <- -coef(fit_valleys_late)[2]


```
#plot curve to test 
```{r}
# Generate exponential curves for early and late segments
max_exp_curve_early <- A_max_early * exp(-eta_max_early * time_early)
min_exp_curve_early <- A_min_early * exp(-eta_min_early * time_early)

max_exp_curve_late <- A_max_late * exp(-eta_max_late * time_late)
min_exp_curve_late <- A_min_late * exp(-eta_min_late * time_late)

# Convert data to data frames for ggplot
df_exp_curve_early <- data.frame(Time = time_early, MaxExpCurve = max_exp_curve_early, MinExpCurve = min_exp_curve_early)
df_exp_curve_late <- data.frame(Time = time_late, MaxExpCurve = max_exp_curve_late, MinExpCurve = min_exp_curve_late)

# Peaks and valleys of the original data
df_peaks <- data.frame(Time = t_peaks, Value = y_peaks)
df_valleys <- data.frame(Time = t_valleys, Value = y_valleys)

# Plot the original data and the fitted exponential curves
ggplot(df_MC, aes(x = Time, y = X1)) +
  geom_line(color = "blue", size = 1) +  # original data
  geom_line(data = df_exp_curve_early, aes(x = Time, y = MaxExpCurve), color = "purple", size = 1, linetype = "dashed") +  # Early segment max exponential curve
  geom_line(data = df_exp_curve_early, aes(x = Time, y = MinExpCurve), color = "yellow", size = 1, linetype = "dashed") +  # Early segment min exponential curve
  geom_line(data = df_exp_curve_late, aes(x = Time, y = MaxExpCurve), color = "purple", size = 1, linetype = "dotted") +  # late segment min exponential curve
  geom_line(data = df_exp_curve_late, aes(x = Time, y = MinExpCurve), color = "yellow", size = 1, linetype = "dotted") +  # late segment min exponential curve
  geom_point(data = df_peaks, aes(x = Time, y = Value), color = "red", size = 2, shape = 17) +  # Red points marking peaks
  geom_point(data = df_valleys, aes(x = Time, y = Value), color = "green", size = 2, shape = 18) +  # Green points marking valleys
  labs(title = "X1 Data with Early and Late Exponential Curve Fitting",
       x = "Time",
       y = "X1(t)") +
  theme_minimal() +
  scale_y_continuous(limits = c(0, max(X1) * 1.1))  # Adjust y-axis range to better display data

```

now calculate omega for time <100 and time>=100
```{r}
# For time < 100
# Calculate Delta and omega for early segment (time < 100)
Delta_early <- t_peaks_early[2] - t_peaks_early[1]  # Assuming the first two peaks are used
omega_early <- 2 * pi / Delta_early

# For time >= 100
# Calculate Delta and omega for late segment (time >= 100)
Delta_late <- t_peaks_late[2] - t_peaks_late[1]  # Assuming the first two peaks in the late segment are used
omega_late <- 2 * pi / Delta_late

```
Create a chart to show parameters
```{r}
#install.packages("knitr") # Uncomment this line if knitr is not installed
#install.packages("DT")    # Uncomment this line if DT is not installed
library(knitr)
library(DT)

# Create a data frame
result_table <- data.frame(
  Parameter = c("A_max", "eta_max", "A_min", "eta_min", "Delta", "omega"),
  `time < 100` = c(A_max_early, eta_max_early, A_min_early, eta_min_early, Delta_early, omega_early),
  `time >= 100` = c(A_max_late, eta_max_late, A_min_late, eta_min_late, Delta_late, omega_late)
)

# Display the table
# Method 1: Use knitr::kable to show a static table
kable(result_table, caption = "Comparison Table for Time < 100 and Time >= 100")
datatable(result_table, caption = "Comparison Table for Time < 100 and Time >= 100")
```



```{r}
#initial meeting model function
asym_model_function <- function(t, A_max, eta_max, A_min, eta_min, omega, phi_j) {
  A_max_t <- A_max * exp(-eta_max * t)
  A_min_t <- A_min * exp(-eta_min * t)
  result <- 0.5 * (A_max_t + A_min_t) + 0.5 * (A_max_t - A_min_t) * cos(omega * t + phi_j)
  return(result)
}

```

## Model Fitting for Asymmetrical Oscillatory Model with fixed variance

In this analysis, we fit an asymmetrical oscillatory model to the McManus data(X1), using \( t = 100 \) as the boundary to split the data into two parts: \( t < 100 \) and \( t \geq 100 \). The model is given by:

\[
y_{i,j} \sim N(f_j(t_i), \sigma_i^2), \quad i = 1, \dots, n, \text{ independently}
\]

where the function \( f_j(t) \) is defined as:

\[
f_j(t) = 0.5 (A_{\text{max}}(t) + A_{\text{min}}(t)) + 0.5 (A_{\text{max}}(t) - A_{\text{min}}(t)) \cos(\tau t + \phi_j)
\]

Here, \( A_{\text{max}}(t) = A_{\text{max}} \cdot \exp(-\eta_{\text{max}} \cdot t) \) and \( A_{\text{min}}(t) = A_{\text{min}} \cdot \exp(-\eta_{\text{min}} \cdot t) \) represent the decaying maximum and minimum amplitudes over time, with parameters \( A_{\text{max}}, \eta_{\text{max}}, A_{\text{min}}, \eta_{\text{min}}, \tau \) (frequency), and \( \phi_j \) (phase shift) as the model parameters to be estimated.

\section*{Fitting Procedure}

We use the \texttt{nls} function in R to fit this non-linear model separately for the data segments \( t < 100 \) and \( t \geq 100 \), using appropriate initial parameter values for each segment based on prior analysis.

Each model fit provides the estimated parameters \( A_{\text{max}}, \eta_{\text{max}}, A_{\text{min}}, \eta_{\text{min}}, \omega, \phi_j \) and their associated 99% confidence intervals.

```{r}
# Split the data into two parts: time < 100 and time >= 100
df_MC_early <- subset(df_MC, Time < 100)
df_MC_late <- subset(df_MC, Time >= 100)

# Define initial parameters for the time < 100 case
initial_params_early <- list(
  A_max = A_max_early,       
  eta_max = eta_max_early,   
  A_min = A_min_early,       
  eta_min = eta_min_early,   
  omega = omega_early,       
  phi_j = 0                  # Initial guess for phi_j
)

# Fit the model for time < 100
fit_nls_early <- nls(
  X1 ~ asym_model_function(Time, A_max, eta_max, A_min, eta_min, omega, phi_j),
  start = initial_params_early,
  data = df_MC_early
)
conf_intervals_early <- confint2(fit_nls_early, level = 0.99, method = 'asymptotic')
cat("Fit results for time < 100:\n")
coef(fit_nls_early)

```

```{r}
# Define initial parameters for the time >= 100 case
initial_params_late <- list(
  A_max = A_max_late,       
  eta_max = eta_max_late,   
  A_min = A_min_late,       
  eta_min = eta_min_late,   
  omega = omega_late,       
  phi_j = 0                 # Initial guess for phi_j
)

# Fit the model for time >= 100
fit_nls_late <- nls(
  X1 ~ asym_model_function(Time, A_max, eta_max, A_min, eta_min, omega, phi_j),
  start = initial_params_late,
  data = df_MC_late
)

cat("Fit results for time >= 100:\n")
summary(fit_nls_late)
conf_intervals_late <- confint2(fit_nls_late, level = 0.99, method = 'asymptotic')


```
The code below extracts the estimated parameters (\( A_{\text{max}} \), \( \eta_{\text{max}} \), \( A_{\text{min}} \), \( \eta_{\text{min}} \), \( \omega \), \( \phi_j \)) and their corresponding 99\% confidence intervals for both \textit{early} (time $<$ 100) and \textit{late} (time $\geq$ 100) periods. It combines these values into a single data frame (\texttt{combined\_data}) that lists each parameter with its estimate, lower bound (0.5\%), and upper bound (99.5\%) for both time periods. The purpose is to organize the parameter estimates and confidence intervals in a structured format, enabling easy comparison between the two time periods.

```{r}
estimates_early <- coef(fit_nls_early)
lower_early <- conf_intervals_early[, 1]
upper_early <- conf_intervals_early[, 2]

estimates_late <- coef(fit_nls_late)
lower_late <- conf_intervals_late[, 1]
upper_late <- conf_intervals_late[, 2]


combined_data <- data.frame(
  Parameter = rep(names(estimates_early), 2),  
  Time_Period = rep(c("time < 100", "time >= 100"), each = length(estimates_early)),  
  Estimate = c(estimates_early, estimates_late),  
  Lower_0.5 = c(lower_early, lower_late), 
  Upper_99.5 = c(upper_early, upper_late)  
)


kable(combined_data, caption = "Estimated Parameters with 99% Confidence Intervals for Different Time Periods")


```
I uses the estimated parameters for the \textit{time} $<$ 100 and \textit{time} $\geq$ 100 periods to generate predicted values using the \texttt{asym\_model\_function}. The original data points (in black) for both periods are overlaid with the model's predicted curves (in blue for \textit{early} and red for \textit{late}). The objective is to visually assess how well the model's predictions (based on estimated parameters) match the original data for both periods.


```{r}
df_MC_early$predicted <- asym_model_function(df_MC_early$Time, estimates_early["A_max"], estimates_early["eta_max"], 
                                             estimates_early["A_min"], estimates_early["eta_min"], 
                                             estimates_early["omega"], estimates_early["phi_j"])

df_MC_late$predicted <- asym_model_function(df_MC_late$Time, estimates_late["A_max"], estimates_late["eta_max"], 
                                            estimates_late["A_min"], estimates_late["eta_min"], 
                                            estimates_late["omega"], estimates_late["phi_j"])




ggplot() +
 
  geom_point(data = df_MC_early, aes(x = Time, y = X1), color = "black", size = 0.3) +
  geom_line(data = df_MC_early, aes(x = Time, y = predicted), color = "blue", size = 0.5) +
 
  geom_point(data = df_MC_late, aes(x = Time, y = X1), color = "black", size = 0.3) +
  geom_line(data = df_MC_late, aes(x = Time, y = predicted), color = "red", size = 0.5) +
 
  labs(title = "Comparison of Original Data and Model Fit",
       x = "Time",
       y = "X1") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))
```



Nonlinear Least Squares Model Fitting with Weights
##time specific function

```{r}
# Define the model function
model_function <- function(t, A, gamma, omega, phi, y_shift) {
  A * exp(-gamma * t ) * cos(omega * t + phi) + y_shift
}
# Function to calculate variance at each time point
calculate_variance <- function(replicates) {
  n <- nrow(replicates)
  mean_vals <- rowMeans(replicates)  # Mean for each time point
  variance <- rowSums((replicates - mean_vals)^2) / (n - 1)  # Variance formula
  return(variance)
}
# Weighted nonlinear least squares estimation
weighted_nls <- function(t, replicates, initial_params) {
  variances <- calculate_variance(replicates)  # Variance at each time point

  # Fit the model using nlsLM
  fit <- nlsLM(y ~ model_function(t, A, gamma, omega, phi, y_shift),
               start = initial_params,
               data = list(t = t, y = rowMeans(replicates)),
               weights = 1 / variances)  # Weight by the inverse of variances
  return(fit)
}
# Use data from replicates for fitting
replicates <- df_MC[, c("X1", "X2", "X3", "X4")]
# Calculate variance
observed_variances <- calculate_variance(replicates)
hist(variances, main = "Histogram of Variances",
     xlab = "Variance",
     ylab = "Frequency",
     col = "lightblue",
     border = "black")


```







```{r}
# # Define the Bayesian inference function
bayesian_variance_estimation <- function(observed_variances, n, alpha_prior = 1, beta_prior = 1) {
  # Input parameters:
  # - observed_variances: a vector of observed variances
  # - n: the number of samples at each time point
  # - alpha_prior, beta_prior: parameters for the Gamma prior distribution
  # Create a vector to store the estimated weight
  weights <- numeric(length(observed_variances))
  
  # # Iterate over each observed variance and perform Bayesian inference
  for (i in 1:length(observed_variances)) {
    S2_i <- observed_variances[i]
    
    alpha_post <- alpha_prior + (n - 1) / 2 # Update the posterior parameters
    beta_post <- beta_prior + S2_i * (n - 1) / 2 
    posterior_mean <- alpha_post / beta_post  # Calculate the mean of the posterior distribution 
                                              #  (mean of a Gamma distribution is shape/scale)
    weights[i] <- posterior_mean  # Use the posterior mean as the reliable weight
  }
  return(weights)
}
# # Set prior parameters (can be adjusted as needed)
alpha_prior <- 1.5  # Gamma： shape
beta_prior <- 2   # Gamma： scale
# Number of samples
n <- ncol(replicates)
# Calculate reliable weights using Bayesian inference
weights <- bayesian_variance_estimation(observed_variances, n, alpha_prior, beta_prior)
# Plot a histogram of the adjusted Bayesian weights
hist(weights, main = "Histogram of Bayesian Adjusted Weights",
     xlab = "Adjusted Weight", ylab = "Frequency", col = "lightblue", border = "black")

```







